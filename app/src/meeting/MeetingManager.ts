import { fetchSyncPost } from "../util/fetch";
import { showMessage } from "../dialog/message";

export interface IMeetingStatus {
    isRecording: boolean;
    isTranscribing: boolean;
    duration: number;
    nextUploadCountdown: number;
}

export class MeetingManager {
    private static instance: MeetingManager;
    private audioContext: AudioContext | null = null;
    private processor: ScriptProcessorNode | null = null;
    private source: MediaStreamAudioSourceNode | null = null;
    private pcmBuffer: Float32Array[] = [];
    private audioProcessCount: number = 0;
    private lastAudioCheck: number = 0;
    private totalSamplesCollected: number = 0;
    private intervalTimer: any = null;
    private startTime: number = 0;
    private intervalSeconds: number = 60;
    private statusCallback: ((status: IMeetingStatus) => void) | null = null;
    private countdownSec: number = 0;
    private _isTranscribing: boolean = false;
    private stream: MediaStream | null = null;

    private constructor() { }

    public static getInstance() {
        if (!MeetingManager.instance) {
            MeetingManager.instance = new MeetingManager();
        }
        return MeetingManager.instance;
    }

    public async startRecording(seconds: number = 60) {
        this.intervalSeconds = seconds;
        this.countdownSec = this.intervalSeconds;
        this.pcmBuffer = [];

        try {
            this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });

            // å…³é”®ï¼šå¼ºåˆ¶è®¾ç½®é‡‡æ ·ç‡ä¸º 16000ï¼Œè§£å†³ ASR è¯†åˆ«ç‡é—®é¢˜
            this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
            this.source = this.audioContext.createMediaStreamSource(this.stream);

            // ä½¿ç”¨ ScriptProcessorNode æ”¶é›†åŸå§‹ PCM æ•°æ®
            this.processor = this.audioContext.createScriptProcessor(4096, 1, 1);

            this.processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                // æ·±åº¦æ‹·è´æ•°æ®
                this.pcmBuffer.push(new Float32Array(inputData));

                // è¯Šæ–­ï¼šç»Ÿè®¡æ”¶é›†çš„éŸ³é¢‘æ•°æ®
                this.audioProcessCount++;
                this.totalSamplesCollected += inputData.length;

                // æ¯ 100 æ¬¡æ‰“å°ä¸€æ¬¡è¯Šæ–­ä¿¡æ¯
                if (this.audioProcessCount % 100 === 0) {
                    console.log("Audio process diagnostic:", {
                        processCount: this.audioProcessCount,
                        bufferChunks: this.pcmBuffer.length,
                        totalSamples: this.totalSamplesCollected,
                        estimatedDuration: (this.totalSamplesCollected / 16000).toFixed(2) + "ç§’"
                    });
                }
            };

            this.source.connect(this.processor);
            this.processor.connect(this.audioContext.destination);

            this.startTime = Date.now();
            this.startTimer();
            console.log("Meeting recording started (PCM 16k), interval:", seconds, "s");
        } catch (err) {
            showMessage("æ— æ³•è®¿é—®éº¦å…‹é£: " + err);
            throw err;
        }
    }

    public stopRecording() {
        this.clearTimer();
        if (this.processor) {
            this.processor.onaudioprocess = null;
            this.processor.disconnect();
            this.processor = null;
        }
        if (this.source) {
            this.source.disconnect();
            this.source = null;
        }
        if (this.audioContext) {
            this.audioContext.close();
            this.audioContext = null;
        }
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
            this.stream = null;
        }

        if (this.statusCallback) {
            this.statusCallback({
                isRecording: false,
                isTranscribing: false,
                duration: 0,
                nextUploadCountdown: 0
            });
        }

        console.log("Meeting recording stopped");
    }

    public get isRecording() {
        return this.audioContext?.state === "running";
    }

    public setStatusCallback(cb: (status: IMeetingStatus) => void) {
        this.statusCallback = cb;
    }

    public setInterval(seconds: number) {
        this.intervalSeconds = seconds;
        this.countdownSec = seconds;
    }

    public getInterval() {
        return this.intervalSeconds;
    }

    private startTimer() {
        this.intervalTimer = setInterval(() => {
            const now = Date.now();
            const duration = Math.floor((now - this.startTime) / 1000);
            this.countdownSec--;

            if (this.countdownSec <= 0) {
                this.uploadAndTranscribe();
                this.countdownSec = this.intervalSeconds;
            }

            if (this.statusCallback) {
                this.statusCallback({
                    isRecording: true,
                    isTranscribing: this._isTranscribing,
                    duration: duration,
                    nextUploadCountdown: this.countdownSec
                });
            }
        }, 1000);
    }

    private clearTimer() {
        if (this.intervalTimer) {
            clearInterval(this.intervalTimer);
            this.intervalTimer = null;
        }
    }

    public async uploadAndTranscribe() {
        if (this.pcmBuffer.length === 0) {
            console.warn("No PCM data to upload, skipping transcription");
            return;
        }

        // 1. åˆå¹¶ PCM æ•°æ®å¹¶è½¬æ¢ä¸º WAV æ ¼å¼
        const audioBlob = this.encodeWAV(this.pcmBuffer);

        // è¯Šæ–­ï¼šè®¡ç®—éŸ³é¢‘æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        const totalSamples = this.pcmBuffer.reduce((acc, s) => acc + s.length, 0);
        const estimatedDuration = (totalSamples / 16000).toFixed(2);

        // æ£€æŸ¥éŸ³é‡ (éŸ³é‡è¿‡å°å¯èƒ½æ˜¯éº¦å…‹é£é—®é¢˜æˆ–æˆæƒå¤±æ•ˆ)
        let maxAmp = 0;
        for (const chunk of this.pcmBuffer) {
            for (let i = 0; i < chunk.length; i++) {
                const a = Math.abs(chunk[i]);
                if (a > maxAmp) maxAmp = a;
            }
        }

        console.log("Audio encoding completed:", {
            maxAmplitude: maxAmp.toFixed(4),
            blobSize: audioBlob.size,
            totalSamples: totalSamples,
            estimatedDuration: estimatedDuration + "ç§’"
        });

        if (maxAmp < 0.01) {
            console.warn("Detected very low audio amplitude. Mic might be muted or not working.");
        }

        this.pcmBuffer = []; // æ¸…ç©ºç¼“å†²åŒºç”¨äºä¸‹ä¸€æ¬¡é‡‡é›†

        const formData = new FormData();
        formData.append("audio", audioBlob, `meeting_${Date.now()}.wav`);

        this._isTranscribing = true;
        console.log("Uploading audio for transcription...");

        fetch("/api/meeting/transcribe", {
            method: "POST",
            body: formData,
        }).then(res => res.json())
            .then(response => {
                console.log("Transcription API response:", response);
                if (response.code === 0 && response.data) {
                    console.log("Transcription successful:", {
                        transcription: response.data.transcription,
                        summary: response.data.summary
                    });
                    this.insertTranscriptionToEditor(response.data);
                } else {
                    console.error("Transcription failed:", response.msg);
                    showMessage("è½¬å½•å¤±è´¥: " + response.msg);
                }
            }).catch(err => {
                console.error("Upload error:", err);
                showMessage("è½¬å½•ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ");
            }).finally(() => {
                this._isTranscribing = false;
            });
    }

    private encodeWAV(samples: Float32Array[]) {
        const sampleRate = 16000;
        const totalLength = samples.reduce((acc, s) => acc + s.length, 0);
        const buffer = new ArrayBuffer(44 + totalLength * 2);
        const view = new DataView(buffer);

        const writeString = (offset: number, string: string) => {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        };

        // WAV å¤´éƒ¨ä¿¡æ¯
        writeString(0, 'RIFF');
        view.setUint32(4, 32 + totalLength * 2, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true); // PCM æ ¼å¼
        view.setUint16(22, 1, true); // å•å£°é“
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * 2, true); // Byte rate
        view.setUint16(32, 2, true); // Block align
        view.setUint16(34, 16, true); // Bits per sample
        writeString(36, 'data');
        view.setUint32(40, totalLength * 2, true);

        // å†™å…¥ PCM é‡‡æ ·æ•°æ®
        let offset = 44;
        for (let i = 0; i < samples.length; i++) {
            const sample = samples[i];
            for (let j = 0; j < sample.length; j++) {
                // å°† Float32 (-1.0 åˆ° 1.0) è½¬æ¢ä¸º Int16 (-32768 åˆ° 32767)
                const s = Math.max(-1, Math.min(1, sample[j]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
                offset += 2;
            }
        }

        return new Blob([buffer], { type: 'audio/wav' });
    }

    private insertTranscriptionToEditor(data: { transcription: string, summary: string }) {
        if (!data.transcription) return;

        // ä¸¥æ ¼è¿‡æ»¤å¤§æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼ˆåŒ…æ‹¬æœªé—­åˆçš„æ ‡ç­¾ï¼‰
        let cleanSummary = data.summary
            .replace(/<think>[\s\S]*?<\/think>/gi, "")
            .replace(/<think>[\s\S]*/gi, "")
            .replace(/<\/think>/gi, "")
            .trim();
        const cleanTranscription = data.transcription
            .replace(/<think>[\s\S]*?<\/think>/gi, "")
            .replace(/<think>[\s\S]*/gi, "")
            .trim();

        // è§£æä¼šè®®çºªè¦ä¸‰è¡Œå†…å®¹
        const parsedSummary = this.parseMeetingSummary(cleanSummary);

        // æ„å»ºç´§å‡‘çš„æ€æºç¬”è®°å—æ ¼å¼
        const timeStr = new Date().toLocaleTimeString('zh-CN', { hour: '2-digit', minute: '2-digit' });

        // ä½¿ç”¨æ€æºç¬”è®°åŸç”Ÿå—æ ¼å¼ï¼Œæ›´ç´§å‡‘
        const content = `> âœ¨ **AI çºªè¦** <span style="opacity: 0.6; font-size: 0.9em;">${timeStr}</span>
> ğŸ“Œ **ä¸»é¢˜**ï¼š${parsedSummary.theme}
> ğŸ’¬ **è¦ç‚¹**ï¼š${parsedSummary.discussion}
> âš¡ **åç»­**ï¼š${parsedSummary.actions}

> ğŸ”½ **è½¬å½•åŸæ–‡**ï¼ˆç‚¹å‡»å±•å¼€ï¼‰
> ${cleanTranscription.split('\n').map(line => '> ' + line).join('\n')}
`;

        const event = new CustomEvent("neura-meeting-transcription", { detail: content });
        window.dispatchEvent(event);
        showMessage("AI è½¬å½•å·²å®æ—¶åŒæ­¥");
    }

    /**
     * è§£æä¼šè®®çºªè¦ä¸‰è¡Œå†…å®¹ä¸ºç»“æ„åŒ–æ•°æ®
     */
    private parseMeetingSummary(summary: string): { theme: string, discussion: string, actions: string } {
        // é»˜è®¤å€¼
        const result = {
            theme: "æœªæå–åˆ°ä¸»é¢˜",
            discussion: "æœªæå–åˆ°è¦ç‚¹",
            actions: "æœªæå–åˆ°åç»­"
        };

        if (!summary) return result;

        // æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
        const lines = summary.split('\n').map(line => line.trim()).filter(line => line.length > 0);

        // è§£ææ¯ä¸€è¡Œï¼Œç§»é™¤ markdown æ ‡è®°
        for (const line of lines) {
            const cleanLine = line
                .replace(/^\s*[-*>]+\s*/g, '')  // ç§»é™¤åˆ—è¡¨æ ‡è®°å’Œå¼•å·
                .replace(/^\*\*[^*]+\*\*[:ï¼š]?\s*/g, '')  // ç§»é™¤ **ä¸»é¢˜ï¼š** è¿™æ ·çš„å‰ç¼€
                .trim();

            // åŒ¹é…ç¬¬ä¸€è¡Œï¼ˆä¸»é¢˜/ä¼šè®®ä¸»é¢˜ï¼‰
            if (line.includes('ä¸»é¢˜') || line.includes('ä¼šè®®ä¸»é¢˜')) {
                result.theme = cleanLine || result.theme;
            }
            // åŒ¹é…ç¬¬äºŒè¡Œï¼ˆè¦ç‚¹/è®¨è®º/å…³é”®è®¨è®ºï¼‰
            else if (line.includes('è¦ç‚¹') || line.includes('è®¨è®º') || line.includes('å…³é”®è®¨è®º')) {
                result.discussion = cleanLine || result.discussion;
            }
            // åŒ¹é…ç¬¬ä¸‰è¡Œï¼ˆåç»­/è¡ŒåŠ¨/è¡ŒåŠ¨é¡¹/å†³è®®ï¼‰
            else if (line.includes('åç»­') || line.includes('è¡ŒåŠ¨') || line.includes('è¡ŒåŠ¨é¡¹') || line.includes('å†³è®®') || line.includes('ç»“è®º')) {
                result.actions = cleanLine || result.actions;
            }
        }

        // å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ ¼å¼ï¼ŒæŒ‰é¡ºåºåˆ†é…
        if (result.theme === "æœªæå–åˆ°ä¸»é¢˜" && lines.length > 0) {
            result.theme = lines[0].replace(/^\s*[-*>]+\s*/g, '').replace(/^\*\*[^*]+\*\*[:ï¼š]?\s*/g, '').trim() || lines[0];
        }
        if (result.discussion === "æœªæå–åˆ°è¦ç‚¹" && lines.length > 1) {
            result.discussion = lines[1].replace(/^\s*[-*>]+\s*/g, '').replace(/^\*\*[^*]+\*\*[:ï¼š]?\s*/g, '').trim() || lines[1];
        }
        if (result.actions === "æœªæå–åˆ°åç»­" && lines.length > 2) {
            result.actions = lines[2].replace(/^\s*[-*>]+\s*/g, '').replace(/^\*\*[^*]+\*\*[:ï¼š]?\s*/g, '').trim() || lines[2];
        }

        return result;
    }
}
